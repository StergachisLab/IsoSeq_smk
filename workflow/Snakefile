import os
import yaml
import glob


# Load config file
configfile: os.environ.get("SNAKEMAKE_CONFIG", "config/github.config.yaml")


run_isoranker = config.get("run_isoranker", False)
output_dir = config.get("output_dir", "Output")

# List of all individuals from the configuration file
individuals = config["individuals"].keys()

# Get existing conditions per individual (skip missing ones or empty dicts)
conditions_per_individual = {
    ind: [
        cond
        for cond in ["treated", "untreated"]
        if cond in config["individuals"][ind]
        and isinstance(config["individuals"][ind][cond], dict)
        and len(config["individuals"][ind][cond]) > 0
    ]
    for ind in individuals
}

# Dictionary containing labels for each individual and condition
labels = {
    ind: {
        cond: list(config["individuals"][ind][cond].keys())
        for cond in conditions_per_individual[ind]
    }
    for ind in individuals
}

# Define valid combinations of individual, condition, and label
combinations = []
for ind in individuals:
    for cond in conditions_per_individual[ind]:
        for lbl in labels[ind][cond]:
            files = config["individuals"][ind][cond][lbl]
            if isinstance(files, list) and len(files) > 0:
                combinations.append(
                    {"individual": ind, "condition": cond, "label": lbl}
                )

# Debug: show combinations being processed
# print("Valid combinations to be processed:")
# for comb in combinations:
#    print(f"{comb['individual']} - {comb['condition']} - {comb['label']}")


# Restrict the values wildcards can take
wildcard_constraints:
    individual="|".join(individuals),
    condition="|".join(
        sorted(
            set(cond for conds in conditions_per_individual.values() for cond in conds)
        )
    ),
    label="|".join(
        sorted(
            set(
                lbl
                for ind in individuals
                for cond in conditions_per_individual[ind]
                for lbl in labels[ind][cond]
            )
        )
    ),


include: "rules/common.smk"


combinations_dict = {
    "individual": [c["individual"] for c in combinations],
    "condition": [c["condition"] for c in combinations],
    "label": [c["label"] for c in combinations],
}

isoranker_outputs = [
    f"{output_dir}/browser/combined_results/merged_ranked_gene_with_phenotype.tsv.gz",
    f"{output_dir}/browser/combined_results/merged_ranked_isoform_with_phenotype.tsv.gz",
    f"{output_dir}/browser/lookup_tables/gene_coverage_lookup_table.tsv.gz",
    f"{output_dir}/browser/lookup_tables/sample_gene_rankings_lookup_table.tsv.gz",
    f"{output_dir}/intermediate/merged_ranked_gene.tsv.gz",
    f"{output_dir}/intermediate/merged_ranked_isoform.tsv.gz",
    f"{output_dir}/qc/gene_diversity.tsv.gz",
    f"{output_dir}/qc/isoform_diversity.tsv.gz",
    f"{output_dir}/qc/pca_plot.pdf",
]


rule all:
    input:
        expand(
            "tag/{individual}_{condition}_{label}.tagged.bam", zip, **combinations_dict
        ),
        expand(
            "qc/{individual}_{condition}_{label}_qc_summary.tsv",
            zip,
            **combinations_dict,
        ),
        expand("qc/{individual}_{condition}_{label}_plots", zip, **combinations_dict),
        *(["collapse/collapsed.gff"] if combinations else []),
        *(["tag/dictionary.tsv.gz"] if combinations else []),
        *(["pigeon/sorted.gff"] if combinations else []),
        *(["pigeon/pigeon_classification.txt"] if combinations else []),
        *(isoranker_outputs if run_isoranker else []),


rule merge_individual_condition:
    conda:
        "envs/env.yml"
    input:
        get_merge_input,
    log:
        "logs/merge_individual_condition/{individual}_{condition}_{label}_merged.log",
    output:
        bam="merged/{individual}_{condition}_{label}_merged.bam",
    threads: config.get("threads", 4)
    resources:
        runtime=480,
    params:
        max_reads=config["max_reads"],
        downsample_reads=config["downsample_reads"],
    benchmark:
        "benchmark/merge_individual_condition/{individual}_{condition}_{label}.txt"
    shell:
        r"""
        set -euo pipefail

        max_reads={params.max_reads}
        downsample_reads={params.downsample_reads}

        input_files=({input})
        num_files=${{#input_files[@]}}

        if [ $num_files -eq 1 ]; then
            # Only one file, just copy it
            cp {input} {output.bam}
        else
            # Merge multiple files
            samtools merge -@ {threads} {output.bam} {input}
        fi

        # Check total reads in the resulting BAM file
        total_reads=$(samtools view -c {output.bam})
        echo "Total reads in merged BAM: $total_reads"

        # Downsampling if needed
        if (( $total_reads > $max_reads )); then
            echo "File {output.bam} has more than $max_reads reads ($total_reads). Downsampling to ~$downsample_reads reads..."
            temp_downsampled_bam=$(mktemp)
            fraction=$(echo "scale=6; $downsample_reads / $total_reads" | bc)
            fraction=$(echo "$fraction" | sed 's/^0//')
            samtools view -s 42$fraction -b {output.bam} > "$temp_downsampled_bam"
            mv "$temp_downsampled_bam" {output.bam}
            echo "Downsampling completed and replaced the original merged BAM."
        else
            echo "File {output.bam} has less than $max_reads reads. No downsampling performed."
        fi
        """


rule label_reads_with_condition:
    conda:
        "envs/env.yml"
    input:
        bam="merged/{individual}_{condition}_{label}_merged.bam",
    output:
        bam="merged/{individual}_{condition}_{label}_labeled.bam",
    log:
        "logs/label_reads_with_condition/{individual}_{condition}_{label}_labeled.log",
    benchmark:
        "benchmark/label_reads_condition/{individual}_{condition}_{label}.txt"
    threads: config.get("threads", 4)
    resources:
        runtime=480,
    shell:
        """
        samtools view -@ {threads} -h {input.bam} | \
        awk -v id="{wildcards.individual}_{wildcards.condition}_{wildcards.label}" 'BEGIN {{OFS="\t"}} !/^@/ {{$1=id"_"$1; print}} /^@/ {{print}}' | \
        samtools view -bS - > {output.bam}
        """


rule align_sample:
    conda:
        "envs/env.yml"
    input:
        bam="merged/{individual}_{condition}_{label}_labeled.bam",
    output:
        aligned="aligned/{individual}_{condition}_{label}_aligned.bam",
    log:
        "logs/align_sample/{individual}_{condition}_{label}_aligned.log",
    threads: config.get("threads", 4)
    resources:
        runtime=480,
        mem_mb=130000,
    params:
        reference=config["reference_genome"],
    benchmark:
        "benchmark/align_samples/{individual}_{condition}_{label}.txt"
    shell:
        """
        pbmm2 align {params.reference} {input.bam} {output.aligned} \
            --preset ISOSEQ --sort -j {threads} --sort-memory 4G --log-level INFO
        """


rule modify_rg:
    conda:
        "envs/env.yml"
    input:
        bam="aligned/{individual}_{condition}_{label}_aligned.bam",
    output:
        bam="aligned/{individual}_{condition}_{label}_modified_aligned.bam",
        bai="aligned/{individual}_{condition}_{label}_modified_aligned.bam.bai",
    log:
        "logs/modify_rg/{individual}_{condition}_{label}_mod_aligned.log",
    threads: config.get("threads", 4)
    benchmark:
        "benchmark/modify_rg/{individual}_{condition}_{label}_rg.txt"
    resources:
        runtime=480,
    shell:
        """
        samtools addreplacerg -r '@RG\\tID:rg1\\tSM:UnnamedSample\\tLB:lib1\\tPL:PACBIO' \
            -o {output.bam} {input.bam}
        samtools index {output.bam}
        """


rule create_dummy_vcf:
    conda:
        "envs/env.yml"
    output:
        "mod_vcf/{individual}_empty.vcf.gz",
    log:
        "logs/create_dummy_vcf/{individual}_empty.log",
    shell:
        r"""
        mkdir -p $(dirname {output})
        (
            echo "##fileformat=VCFv4.2"
            printf "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tUnnamedSample\n"
        ) | bgzip -c > {output}
        """


rule modify_vcf:
    conda:
        "envs/env.yml"
    input:
        vcf=get_vcf_path,
    output:
        mod_vcf="mod_vcf/{individual}_mod.vcf.gz",
        mod_vcf_tbi="mod_vcf/{individual}_mod.vcf.gz.tbi",
    log:
        "logs/modify_vcf/{individual}_mod_vcf.log",
    threads: config.get("threads", 2)
    benchmark:
        "benchmark/modify_vcf/{individual}_mod_vcf.txt"
    shell:
        r"""
        if [[ "$(basename {input.vcf})" == *_empty.vcf.gz ]] || [ ! -s {input.vcf} ]; then
            # No VCF provided, use dummy already created
            cp {input.vcf} {output.mod_vcf}
            tabix -p vcf {output.mod_vcf}
        else
            (zcat {input.vcf} 2>/dev/null || cat {input.vcf}) \
            | bcftools view \
            | awk 'BEGIN {{FS=OFS="\t"}} /^#CHROM/ {{$10="UnnamedSample"; print; next}} {{print}}' \
            | bgzip -@ {threads} -c > {output.mod_vcf} && \
            tabix -p vcf {output.mod_vcf}
        fi
        """


rule whatshap_haplotag:
    conda:
        "envs/env.yml"
    resources:
        runtime=480,
        mem_mb=130000,
    input:
        bam="aligned/{individual}_{condition}_{label}_modified_aligned.bam",
        phased_vcf_gz="mod_vcf/{individual}_mod.vcf.gz",
        reference=config["reference_genome"],
    output:
        haplotagged_bam="whatshap/{individual}_{condition}_{label}.haplotagged.bam",
        list_txt="whatshap/{individual}_{condition}_{label}.list.txt",
    log:
        "logs/whatshap_haplotag/{individual}_{condition}_{label}.haplotag.log",
    params:
        script=workflow.source_path("scripts/add_hp0.sh"),
    benchmark:
        "benchmark/whatshap/{individual}_{condition}_{label}_haplotag.txt"
    shell:
        """
        if [ $(zcat {input.phased_vcf_gz} | grep -vc '^#') -gt 0 ]; then
            echo "Variants found. Running whatshap haplotag."
            whatshap haplotag {input.phased_vcf_gz} {input.bam} \
                -o {output.haplotagged_bam} --reference {input.reference} \
                --output-haplotag-list {output.list_txt}
        else
            echo "No variants found or condition failed. Running add_hp0.sh."
            sh {params.script} {input.bam} {output.haplotagged_bam}
            echo -e "#readname\thaplotype\tphaseset\tchromosome" > {output.list_txt}
        fi
        """


rule extract_read_info:
    conda:
        "envs/env.yml"
    input:
        bam="whatshap/{individual}_{condition}_{label}.haplotagged.bam",
    output:
        txt="whatshap/{individual}_{condition}_{label}.haplotagged.txt",
    log:
        "logs/extract_read_info/{individual}_{condition}_{label}.haplotag.log",
    benchmark:
        "benchmark/extract_read_info/{individual}_{condition}_{label}.txt"
    resources:
        runtime=480,
    params:
        script=workflow.source_path("scripts/extract_read_info.py"),
    shell:
        """
        python {params.script} {input.bam} {output.txt}
        """


rule merge_samples:
    conda:
        "envs/env.yml"
    input:
        bam=combine_labels,
    output:
        bam="merged/{individual}_all_conditions_merged.bam",
        bai="merged/{individual}_all_conditions_merged.bam.bai",
    log:
        "logs/merge_samples/{individual}_conditions_merged.log",
    threads: config.get("threads", 4)
    resources:
        runtime=480,
    benchmark:
        "benchmark/merge_samples/{individual}_all_conditions_merged.txt"
    shell:
        """
        samtools merge -@ {threads} {output.bam} {input.bam}
        samtools index {output.bam}
        """


rule cluster:
    conda:
        "envs/env.yml"
    input:
        bam="merged/{individual}_all_conditions_merged.bam",
        bai="merged/{individual}_all_conditions_merged.bam.bai",
    log:
        "logs/cluster/{individual}_all_conditions_merged.log",
    output:
        clustered="clustered/{individual}_clustered.bam",
    threads: 20
    resources:
        runtime=1800,
        mem_mb=550000,
    benchmark:
        "benchmark/cluster/{individual}_all_conditions.txt"
    shell:
        """
        isoseq cluster2 {input.bam} {output.clustered} -j {threads} --singletons
        """


rule align:
    conda:
        "envs/env.yml"
    input:
        "clustered/{individual}_clustered.bam",
    output:
        aligned="aligned/{individual}_aligned.bam",
    log:
        "logs/align/{individual}_aligned.log",
    threads: config.get("threads", 4)
    resources:
        runtime=480,
        mem_mb=130000,
    params:
        reference=config["reference_genome"],
    benchmark:
        "benchmark/align/{individual}_aligned.txt"
    shell:
        """
        pbmm2 align {params.reference} {input} {output.aligned} \
            --preset ISOSEQ --sort -j {threads} --sort-memory 4G --log-level INFO
        """


rule label_transcripts:
    conda:
        "envs/env.yml"
    input:
        bam="aligned/{individual}_aligned.bam",
    output:
        bam="aligned/{individual}_aligned_labeled.bam",
    log:
        "logs/label_transcripts/{individual}_aligned_labeled.log",
    threads: config.get("threads", 4)
    benchmark:
        "benchmark/label_transcripts/{individual}_aligned_labeled.txt"
    resources:
        runtime=480,
        mem_mb=80000,
    shell:
        r"""
        (
            samtools view -@ {threads} -h {input.bam} \
            | awk -v id={wildcards.individual} 'BEGIN {{OFS="\t"}} !/^@/ {{$1=id"_"$1; print}} /^@/ {{print}}'
        ) | samtools view -@ {threads} -bS - > {output.bam}
        """


rule merge_all_aligned:
    conda:
        "envs/env.yml"
    input:
        lambda wildcards: expand(
            "aligned/{individual}_aligned_labeled.bam",
            individual=config["individuals"].keys(),
        ),
    output:
        "aligned/all_individuals_aligned_merged.bam",
    log:
        "logs/merge_all_aligned/all_individuals_aligned_merged.log",
    threads: config.get("threads", 4)
    resources:
        runtime=480,
        mem_mb=80000,
    benchmark:
        "benchmark/merge_all_aligned/all_aligned_merged.txt"
    shell:
        """
        samtools merge -@ {threads} {output} {input}
        samtools index {output}
        """


rule collapse:
    conda:
        "envs/env.yml"
    input:
        "aligned/all_individuals_aligned_merged.bam",
    output:
        collapsed_gff="collapse/collapsed.gff",
        collapsed_abundance="collapse/collapsed.abundance.txt",
        collapsed_readstat="collapse/collapsed.read_stat.txt",
        collapsed_count="collapse/collapsed.flnc_count.txt",
    log:
        "logs/collapse/collapsed.log",
    threads: config.get("threads", 4)
    resources:
        runtime=480,
        mem_mb=80000,
    benchmark:
        "benchmark/collapse/collapse.txt"
    shell:
        """
        isoseq collapse --num-threads {threads} {input} {output.collapsed_gff}
        """


rule pigeon_prepare_annot:
    conda:
        "envs/env.yml"
    input:
        gtf=config["pigeon_annot"],
    output:
        gtf="annotations/annotations.sorted.gtf",
        pgi="annotations/annotations.sorted.gtf.pgi",
    log:
        "logs/pigeon_prepare_annot/pigeon_prepare.log",
    benchmark:
        "benchmark/pigeon_prepare/annotation_sort.txt"
    shell:
        r"""
        if file {input.gtf} | grep -q 'gzip compressed'; then
            gunzip -c {input.gtf} > annotations/annotations.gtf
        else
            cp {input.gtf} annotations/annotations.gtf
        fi

        pigeon prepare annotations/annotations.gtf       
        """


rule pigeon_process:
    conda:
        "envs/env.yml"
    input:
        collapsed_gff="collapse/collapsed.gff",
        collapsed_count="collapse/collapsed.flnc_count.txt",
        prepared_gtf="annotations/annotations.sorted.gtf",
        prepared_index="annotations/annotations.sorted.gtf.pgi",
    output:
        sorted_gff="pigeon/sorted.gff",
        classification="pigeon/pigeon_classification.txt",
        report="pigeon/saturation.txt",
    log:
        "logs/pigeon_process/pigeon_process.log",
    params:
        reference=config["reference_genome"],
    benchmark:
        "benchmark/pigeon_process/pigeon.txt"
    threads: config.get("threads", 4)
    resources:
        runtime=480,
        mem_mb=80000,
    shell:
        """
        pigeon sort {input.collapsed_gff} -o {output.sorted_gff}
        pigeon classify {output.sorted_gff} {input.prepared_gtf} {params.reference} \
            --flnc {input.collapsed_count} -d pigeon -o 'pigeon'
        pigeon filter {output.classification}
        pigeon report pigeon/pigeon_classification.filtered_lite_classification.txt {output.report}
        """


rule create_dictionary:
    conda:
        "envs/r-base.yml"
    input:
        collapsed="collapse/collapsed.read_stat.txt",
        haplotag=get_haplotag_files,
        classification="pigeon/pigeon_classification.txt",
    output:
        "tag/dictionary.tsv.gz",
    log:
        "logs/create_dictionary/dictionary.log",
    benchmark:
        "benchmark/create_dictionary/dictionary.txt"
    threads: config.get("threads", 4)
    resources:
        runtime=980,
        mem_mb=500000,
    params:
        script=workflow.source_path("scripts/create_dictionary.R"),
    shell:
        """
        Rscript {params.script} {input.collapsed} whatshap/ {input.classification} {threads} tag > {log} 2>&1
        """


rule add_tags_to_bam:
    conda:
        "envs/env.yml"
    input:
        bam="whatshap/{individual}_{condition}_{label}.haplotagged.bam",
        dictionary="tag/dictionary.tsv.gz",
    output:
        bam="tag/{individual}_{condition}_{label}.tagged.bam",
        bai="tag/{individual}_{condition}_{label}.tagged.bam.bai",
    log:
        "logs/add_tags_to_bam/{individual}_{condition}_{label}.tagged.log",
    benchmark:
        "benchmark/add_tags_to_bam/{individual}_{condition}_{label}_tagged.txt"
    threads: config.get("threads", 8)
    resources:
        runtime=480,
        mem_mb=15000,
    params:
        script=workflow.source_path("scripts/add_tags_to_bam.sh"),
    shell:
        """
        bash {params.script} {input.bam} {input.dictionary} {output.bam} {threads}
        """


rule qc_flnc:
    conda:
        "envs/python-env.yml"
    input:
        bam="tag/{individual}_{condition}_{label}.tagged.bam",
    output:
        tsv="qc/{individual}_{condition}_{label}_qc_summary.tsv",
        plots_dir=directory("qc/{individual}_{condition}_{label}_plots"),
    log:
        "logs/qc_flnc/{individual}_{condition}_{label}.qc.log",
    params:
        output_dir="qc/{individual}_{condition}_{label}_plots",
        script=workflow.source_path("scripts/qc_metrics.py"),
    benchmark:
        "benchmark/qc_flnc/{individual}_{condition}_{label}_qc.txt"
    shell:
        """
        python {params.script} {input.bam} {output.tsv} {params.output_dir}
        """


docs_dir = config["docs_dir"]
reference_genome = config["reference_genome"]


rule isoranker_analysis:
    input:
        read_stat="collapse/collapsed.read_stat.txt",
        sample_info=os.path.join(docs_dir, "Sample_info.tsv"),
        classification="pigeon/pigeon_classification.txt",
        genemap=os.path.join(docs_dir, "genemap2.txt"),
        hpo_file=os.path.join(docs_dir, "phenotype.hpoa"),
        probands_file=os.path.join(
            docs_dir, "Multiome_samples_clinical_findings_2.11.25.tsv"
        ),
    output:
        gene_with_phenotype=f"{output_dir}/browser/combined_results/merged_ranked_gene_with_phenotype.tsv.gz",
        isoform_with_phenotype=f"{output_dir}/browser/combined_results/merged_ranked_isoform_with_phenotype.tsv.gz",
        gene_lookup=f"{output_dir}/browser/lookup_tables/gene_coverage_lookup_table.tsv.gz",
        ranking_lookup=f"{output_dir}/browser/lookup_tables/sample_gene_rankings_lookup_table.tsv.gz",
        merged_gene=f"{output_dir}/intermediate/merged_ranked_gene.tsv.gz",
        merged_isoform=f"{output_dir}/intermediate/merged_ranked_isoform.tsv.gz",
        gene_div=f"{output_dir}/qc/gene_diversity.tsv.gz",
        isoform_div=f"{output_dir}/qc/isoform_diversity.tsv.gz",
        pca_plot=f"{output_dir}/qc/pca_plot.pdf",
    threads: config.get("threads", 4)
    log:
        f"logs/isoranker_analysis/isoranker.log",
    resources:
        runtime=480,
        mem_mb=500000,
    benchmark:
        f"benchmark/isoranker.txt"
    params:
        output_dir=lambda wildcards, output: os.path.commonpath(list(output)),
        ref_fa=reference_genome,
    conda:
        "envs/isoranker.yml"
    shell:
        """
        isoranker_pb_run_analysis \
          --read_stat_path {input.read_stat} \
          --reference_fasta_path {params.ref_fa} \
          --sample_info_path {input.sample_info} \
          --classification_path {input.classification} \
          --genemap_path {input.genemap} \
          --hpo_file_path {input.hpo_file} \
          --probands_file_path {input.probands_file} \
          --final_output_dir {params.output_dir}
        """
